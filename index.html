<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <!-- Meta tags for social media banners, these should be filled in appropriatly as they are your "business card" -->
  <!-- Replace the content tag with appropriate information -->
  <meta name="description" content="DESCRIPTION META TAG">
  <meta property="og:title" content="SOCIAL MEDIA TITLE TAG"/>
  <meta property="og:description" content="SOCIAL MEDIA DESCRIPTION TAG TAG"/>
  <meta property="og:url" content="URL OF THE WEBSITE"/>
  <!-- Path to banner image, should be in the path listed below. Optimal dimenssions are 1200X630-->
  <meta property="og:image" content="static/image/your_banner_image.png" />
  <meta property="og:image:width" content="1200"/>
  <meta property="og:image:height" content="630"/>


  <meta name="twitter:title" content="TWITTER BANNER TITLE META TAG">
  <meta name="twitter:description" content="TWITTER BANNER DESCRIPTION META TAG">
  <!-- Path to banner image, should be in the path listed below. Optimal dimenssions are 1200X600-->
  <meta name="twitter:image" content="static/images/your_twitter_banner_image.png">
  <meta name="twitter:card" content="summary_large_image">
  <!-- Keywords for your paper to be indexed by-->
  <meta name="keywords" content="KEYWORDS SHOULD BE PLACED HERE">
  <meta name="viewport" content="width=device-width, initial-scale=1">


  <title>Spatio-Temporal LLM: Reasoning about Environments and Actions</title>
  <link rel="icon" type="image/x-icon" href="static/images/stllm_icon_nobg.png">
  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro"
  rel="stylesheet">

  <link rel="stylesheet" href="static/css/bulma.min.css">
  <link rel="stylesheet" href="static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="static/css/fontawesome.all.min.css">
  <link rel="stylesheet"
  href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="static/css/index.css">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script src="https://documentcloud.adobe.com/view-sdk/main.js"></script>
  <script defer src="static/js/fontawesome.all.min.js"></script>
  <script src="static/js/bulma-carousel.min.js"></script>
  <script src="static/js/bulma-slider.min.js"></script>
  <script src="static/js/index.js"></script>
</head>
<body>


  <section class="hero">
    <div class="hero-body">
      <div class="container is-max-desktop">
        <div class="columns is-centered">
          <div class="column has-text-centered">
            <h1 class="title is-1 publication-title">
              <img src="static/images/stllm_icon_nobg.png" alt="icon" style="height:1.2em;vertical-align:middle;margin-right:0.1em;">
              Spatio-Temporal LLM: Reasoning about Environments and Actions
            </h1>
            <div class="is-size-5 publication-authors">
              <!-- Paper authors -->
              <span class="author-block">
                <a target="_blank">Haozhen Zheng</a>,</span>
                <span class="author-block">
                  <a href="https://www.beitongtian.com/" target="_blank">Beitong Tian</a>,</span>
                  <span class="author-block">
                    <a href="https://scholar.google.com/citations?user=kp3PK7IAAAAJ&hl=en" target="_blank">Mingyuan Wu</a>,</span>
                    <span class="author-block">
                      <a href="https://recordmp3.github.io/" target="_blank">Zhenggang Tang</a>,</span> 
                      <span class="author-block">
                        <a href="https://siebelschool.illinois.edu/about/people/faculty/klara" target="_blank">Klara Nahrstedt</a>,</span> 
                        <span class="author-block">
                          <a href="https://alexander-schwing.de/" target="_blank">Alex Schwing</a></span> 
                    
                  </div>

                  <div class="is-size-5 publication-authors">
                    <span class="author-block">University of Illinois Urbana-Champaign<br>Preprint</span>
                  </div>

                  <div class="column has-text-centered">
                    <div class="publication-links">
                         <!-- Arxiv PDF link -->
                      <span class="link-block">
                        <a href="https://arxiv.org/abs/2507.05258" target="_blank"
                        class="external-link button is-normal is-rounded is-dark">
                        <span class="icon">
                          <i class="fas fa-file-pdf"></i>
                        </span>
                        <span>Paper</span>
                      </a>
                    </span>

                    <!-- Data link -->
                    <span class="link-block">
                      <a href="https://drive.google.com/drive/folders/1qX9Pn50NFR_dNuz6eH3TnQPZDtwQu0W8?usp=sharing" target="_blank"
                      class="external-link button is-normal is-rounded is-dark">
                      <span class="icon">
                        <i class="fas fa-database"></i>
                      </span>
                      <span>Data</span>
                    </a>
                  </span>

                  <!-- Github link -->
                  <span class="link-block">
                    <a href="https://github.com/zoezheng126/Spatio-Temporal-LLM" target="_blank"
                    class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                      <i class="fab fa-github"></i>
                    </span>
                    <span>Code</span>
                  </a>
                </span>

                <!-- ArXiv abstract Link -->
                <span class="link-block">
                  <a href="https://arxiv.org/abs/2507.05258" target="_blank"
                  class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                    <i class="ai ai-arxiv"></i>
                  </span>
                  <span>arXiv</span>
                </a>
              </span>
            </div>
          </div>
        </div>
      </div>
    </div>
  </div>
</section>


<!-- Teaser video-->
<section class="hero teaser">
  <div class="container is-max-desktop">
    <div class="hero-body">
      <img src="static/images/teaser-crop.jpg" alt="Banner Image" id="tree" style="width: 100%; height: auto;">
      <h2 class="subtitle has-text-centered">
        Both spatial and temporal understanding is required to correctly answer questions in our "Reasoning about Environments and Actions" (REA) data.
      </h2>
    </div>
  </div>
</section>
<!-- End teaser video -->

<!-- Paper abstract -->
<section class="section hero is-light">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Abstract</h2>
        <div class="content has-text-justified">
          <p>
            Despite the significant recent progress of Multimodal Large Language Models (MLLMs), MLLMs still struggle to correctly answer prompts that require a holistic spatio-temporal understanding. Specifically, it is challenging to answer prompts that refer to 1) the entirety of an environment that an agent equipped with an MLLM can operate in; and simultaneously also refer to 2) recent actions that just happened and are encoded in a video clip. However, such a holistic spatio-temporal understanding is important for agents operating in the real world. To address this challenge, we first develop a framework to collect a large-scale dataset. Using the collected "Reasoning about Environments and Actions" (REA) dataset, we show that recent methods indeed struggle to correctly answer the prompts. To improve, we develop 
a "spatio-temporal LLM" (ST-LLM), a model equipped with projectors to improve both spatial understanding of an environment and temporal understanding of recent observations. On the collected REA data, we show that the proposed method significantly improves results compared to prior work.
          </p>
        </div>
      </div>
    </div>
  </div>
</section>
<!-- End paper abstract -->


<!-- Image carousel -->
<section class="hero is-small">
  <div class="hero-body">
    <div class="container">
      <h2 class="title is-3 has-text-centered">Dataset</h2>
      <div class="content has-text-justified" style="max-width: 800px; margin: 0 auto 2em auto;">
        <p>
          The Reasoning about Environments and Actions (REA) dataset is designed to benchmark the spatio-temporal reasoning capabilities of multimodal large language models (MLLMs). It contains question-answer (QA) pairs organized into five distinct tasks, each targeting a different aspect of spatial and temporal understanding.
          Answering these questions requires models to reason about both the global 3D scene context (e.g., room layout, object positions) and localized temporal cues from egocentric video (e.g., human-object interactions, motion sequences). Together, these tasks offer a comprehensive challenge for developing MLLMs capable of real-world reasoning.
          The dataset includes 24,445 training samples and 1,757 validation samples, providing a solid foundation for both supervised learning and performance evaluation.
          Below, we showcase qualitative examples from each of the five REA tasks.
        </p>
      </div>
      <div id="results-carousel" class="carousel results-carousel">
        <div class="item">
          <!-- Your image here -->
          <img src="static/images/relative_direction.jpg" alt="Relative direction example" style="display: block; margin: 0 auto; width: 100%; height: auto; max-height: 400px; object-fit: contain;"/>
          <h2 class="subtitle has-text-centered">
            <strong>Relative Direction:</strong> <br>
            <strong>Q:</strong> Is the microwave to the left of the person when the person is performing `put down knives' and also to the left of the person when performing `pick up plate'? <br>
            <strong>A:</strong> The microwave remains to the left of the person during both `put down knives' and `pick up plate'.
          </h2>
        </div>
        <div class="item">
          <!-- Your image here -->
          <img src="static/images/relative_distance.jpg" alt="Relative distance example" style="display: block; margin: 0 auto; width: 100%; height: auto; max-height: 400px; object-fit: contain;"/>
          <h2 class="subtitle has-text-centered">
            <strong>Relative Distance:</strong> <br>
            <strong>Q:</strong> Does the person move closer to the hob between `turn off rice cooker' and `serve curry'? <br>
            <strong>A:</strong> The person remains at about the same distance from the hob when performing both `turn off rice cooker' and `serve curry'.
          </h2>
        </div>
        <div class="item">
          <!-- Your image here -->
          <img src="static/images/find_my_item.jpg" alt="Find my item example" style="display: block; margin: 0 auto; width: 100%; height: auto; max-height: 400px; object-fit: contain;"/>
          <h2 class="subtitle has-text-centered">
            <strong>Find My Item:</strong> <br>
            <strong>Q:</strong> Where is the tarragon, and how can the person get to it? <br>
            <strong>A:</strong> The tarragon is located on the countertop to the right of the person. The person can reach it by turning to their right and walking towards the countertop.
          </h2>
        </div>
        <div class="item">
          <!-- Your image here -->
          <img src="static/images/furniture_affordance.jpg" alt="Furniture affordance example" style="display: block; margin: 0 auto; width: 100%; height: auto; max-height: 400px; object-fit: contain;"/>
          <h2 class="subtitle has-text-centered">
            <strong>Furniture Affordance:</strong> <br>
            <strong>Q:</strong> Which of the following objects does the person interact with next, given their previous actions and current motion? A. hob, B. oven, C. fridge <br>
            <strong>A:</strong> The person is most likely to interact with the hob next because they have just finished cleaning the sink and are now moving towards the hob, which suggests they might need to use it for cooking or cleaning purposes.
          </h2>
        </div>
        <div class="item">
          <!-- Your image here -->
          <img src="static/images/action_planning.jpg" alt="Action planning example" style="display: block; margin: 0 auto; width: 100%; height: auto; max-height: 400px; object-fit: contain;"/>
          <h2 class="subtitle has-text-centered">
            <strong>Action Planning:</strong> <br>
            <strong>Q:</strong> We are performing a cooking/assembly task with the following sequence of actions: put down tray, put down sponge, take glass, wash knife.. Based on the video, what should I do next, and how can I get to the place where the next step takes place? <br>
            <strong>A:</strong> You have already completed the actions of putting down the tray and the sponge. Now, you are about to take the glass and wash the knife. To do this, move right to the sink and start washing the knife.
          </h2>
        </div>
      </div>
    </div>
  </div>
</section>
<!-- End image carousel -->

<!-- Data Generation Pipeline -->
<section class="hero is-small is-light">
  <div class="hero-body">
    <div class="container">
      <div class="columns is-centered">
        <div class="column is-full">
          <div class="box has-background-white pipeline-box-custom">
            <h2 class="title is-3 has-text-centered" style="margin-bottom: 1.5rem;">Data Generation Pipeline</h2>
            <p class="content">
              The REA dataset is constructed via a six-step pipeline that integrates egocentric videos with 3D scene understanding.
              For each query, we sample video clips from <em>EPIC-KITCHENS</em> and estimate the 3D positions of the person and objects
              using action annotations and segmentation masks. We then compute spatial relationships, refine navigation paths with
              a VideoLLM, and reconstruct a dense point cloud using hand-filtered frames. Finally, each video frame is registered
              to the point cloud to enable precise spatial grounding.
            </p>
            <div class="image-slider" id="pipeline-slider">
              <div class="slider-scroll" id="slider-scroll">
                <img src="static/images/pcd_reg-crop.jpg" alt="Data generation pipeline" class="slider-image"/>
              </div>
            </div>
          </div>
        </div>
      </div>
    </div>
  </div>
</section>

<style>
/* Match the border-radius to the Model Architecture section (16px) */
.pipeline-box-custom {
  border-radius: 16px !important;
  box-shadow: 0 2px 16px rgba(0,0,0,0.07);
  padding: 2.5rem 2rem;
  margin: 0 auto;
  max-width: 1100px;
}

.image-slider {
  position: relative;
  width: 100%;
  max-width: 1200px;
  height: 400px;
  margin: 0 auto;
  overflow-x: auto;
  overflow-y: hidden;
  background: #fff;
  border-radius: 10px;
  box-shadow: 0 2px 8px rgba(0,0,0,0.1);
  cursor: grab;
}

.slider-scroll {
  min-width: max-content;
  height: 100%;
  display: flex;
  align-items: center;
}

.slider-image {
  height: 100%;
  width: auto;
  user-select: none;
  pointer-events: none;
  margin-right: 0;
}
</style>

<script>
const slider = document.getElementById('pipeline-slider');
let isDown = false;
let startX;
let scrollLeft;

slider.addEventListener('mousedown', (e) => {
  isDown = true;
  slider.classList.add('active');
  startX = e.pageX - slider.offsetLeft;
  scrollLeft = slider.scrollLeft;
});
slider.addEventListener('mouseleave', () => {
  isDown = false;
  slider.classList.remove('active');
});
slider.addEventListener('mouseup', () => {
  isDown = false;
  slider.classList.remove('active');
});
slider.addEventListener('mousemove', (e) => {
  if (!isDown) return;
  e.preventDefault();
  const x = e.pageX - slider.offsetLeft;
  const walk = (x - startX) * 1.5; // 拖动速度
  slider.scrollLeft = scrollLeft - walk;
});
</script>





<!-- Image presentation -->
<section class="hero is-small is-light">
  <div class="hero-body">
    <div class="container">
      <div style="background: #fff; border-radius: 16px; box-shadow: 0 2px 16px rgba(0,0,0,0.07); padding: 2.5rem 2rem; margin: 0 auto; max-width: 1100px;">
        <h2 class="title is-3 has-text-centered">Model Architecture</h2>

        <p class="content">
          The proposed Spatio-Temporal LLM (ST-LLM) integrates egocentric video, 3D point cloud, and text inputs for unified reasoning.
          It leverages a pretrained vision encoder and a point cloud encoder to extract features, which are aligned using a cross-modal
          Q-Former module with learnable queries. A 3D positional encoding is applied to both video and point cloud features to enhance
          spatial precision. The fused representation is then passed to an LLM decoder to generate task-specific answers.
        </p>

        <div style="width:100%;display:flex;justify-content:center;">
          <img src="static/images/model.jpg" alt="Presentation Image" id="tree" style="width:100%;max-width:1200px;height:auto;">
        </div>
      </div>
    </div>
  </div>
</section>



<!--BibTex citation -->
  <section class="section" id="BibTeX">
    <div class="container is-max-desktop content">
      <h2 class="title">BibTeX</h2>
      <pre><code>@misc{zheng2025spatiotemporalllmreasoningenvironments,
        title={Spatio-Temporal LLM: Reasoning about Environments and Actions}, 
        author={Haozhen Zheng and Beitong Tian and Mingyuan Wu and Zhenggang Tang and Klara Nahrstedt and Alex Schwing},
        year={2025},
        eprint={2507.05258},
        archivePrefix={arXiv},
        primaryClass={cs.CV},
        url={https://arxiv.org/abs/2507.05258}, 
  }</code></pre>
    </div>
</section>
<!--End BibTex citation -->


  <footer class="footer">
  <div class="container">
    <div class="columns is-centered">
      <div class="column is-8">
        <div class="content">

          <p>
            This page was built using the <a href="https://github.com/eliahuhorwitz/Academic-project-page-template" target="_blank">Academic Project Page Template</a> which was adopted from the <a href="https://nerfies.github.io" target="_blank">Nerfies</a> project page.
            You are free to borrow the source code of this website, we just ask that you link back to this page in the footer. <br> This website is licensed under a <a rel="license"  href="http://creativecommons.org/licenses/by-sa/4.0/" target="_blank">Creative
            Commons Attribution-ShareAlike 4.0 International License</a>.
          </p>

        </div>
      </div>
    </div>
  </div>
</footer>

<!-- Statcounter tracking code -->
  
<!-- You can add a tracker to track page visits by creating an account at statcounter.com -->

    <!-- End of Statcounter Code -->

  </body>
  </html>
